{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "source_dir = os.path.join(\"..\", \"arxiv_papers_infos\")\n",
    "papers_paths = glob(os.path.join(source_dir, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "paper_data = []\n",
    "for path in papers_paths:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    paper_data.append(data)\n",
    "print(len(paper_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "titles = []\n",
    "for data in paper_data:\n",
    "    if \"result\" not in data:\n",
    "        continue\n",
    "    data = data[\"result\"]\n",
    "    authors = \"\" if \"authors\" not in data else \" \".join(data[\"authors\"])\n",
    "    title = \"\" if \"title\" not in data else data[\"title\"]\n",
    "    abstract = \"\" if \"abstract\" not in data else data[\"abstract\"]\n",
    "    # sent = authors + \" \" + title + \" \" + abstract\n",
    "    sent = title + \" ----- \" + abstract\n",
    "    docs.append(sent)\n",
    "    titles.append(title)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(docs[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "runs_df = {\n",
    "    \"TSNE perplexity\": [],\n",
    "    \"DBSCAN eps\": [],\n",
    "    \"DBSCAN min_samples\": [],\n",
    "    \"Num. clusters\": [],\n",
    "    \"Num. noise\": [],\n",
    "    \"Silhouette\": [],\n",
    "}\n",
    "\n",
    "for perp in range(10, 210, 10):\n",
    "    X_tsne = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate='auto',\n",
    "        init='random',\n",
    "        perplexity=perp\n",
    "    ).fit_transform(embeddings)\n",
    "    for _eps in range(1, 20):\n",
    "        eps = _eps / 4.0\n",
    "        for min_samples in range(2, 15):\n",
    "            db = DBSCAN(eps=eps, min_samples=min_samples).fit(X_tsne)\n",
    "            labels = db.labels_\n",
    "            if not (1 < len(set(labels)) < len(X_tsne)):\n",
    "                sil_score = -1.0\n",
    "            else:\n",
    "                sil_score = silhouette_score(X_tsne, labels)\n",
    "            n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise_ = list(labels).count(-1)\n",
    "            runs_df[\"TSNE perplexity\"].append(perp)\n",
    "            runs_df[\"DBSCAN eps\"].append(eps)\n",
    "            runs_df[\"DBSCAN min_samples\"].append(min_samples)\n",
    "            runs_df[\"Num. clusters\"].append(n_clusters_)\n",
    "            runs_df[\"Num. noise\"].append(n_noise_)\n",
    "            runs_df[\"Silhouette\"].append(sil_score)\n",
    "\n",
    "runs_df = pd.DataFrame(runs_df)\n",
    "\n",
    "# n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "# n_noise_ = list(labels).count(-1)\n",
    "\n",
    "# print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "# print(f\"Estimated number of noise points: {n_noise_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "runs_df = {\n",
    "    \"TSNE perplexity\": [],\n",
    "    \"Num. clusters\": [],\n",
    "    \"Silhouette\": [],\n",
    "}\n",
    "\n",
    "for perp in range(10, 210, 10):\n",
    "    X_tsne = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate=\"auto\",\n",
    "        init=\"random\",\n",
    "        perplexity=perp\n",
    "    ).fit_transform(embeddings)\n",
    "    for num_clusters in range(2, 30):\n",
    "        db = AgglomerativeClustering(n_clusters=num_clusters, linkage=\"ward\").fit(X_tsne)\n",
    "        labels = db.labels_\n",
    "        if not (1 < len(set(labels)) < len(X_tsne)):\n",
    "            sil_score = -1.0\n",
    "        else:\n",
    "            sil_score = silhouette_score(X_tsne, labels)\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "        runs_df[\"TSNE perplexity\"].append(perp)\n",
    "        runs_df[\"Num. clusters\"].append(num_clusters)\n",
    "        runs_df[\"Silhouette\"].append(sil_score)\n",
    "\n",
    "runs_df = pd.DataFrame(runs_df)\n",
    "\n",
    "# n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "# n_noise_ = list(labels).count(-1)\n",
    "\n",
    "# print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "# print(f\"Estimated number of noise points: {n_noise_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSNE perplexity</th>\n",
       "      <th>Num. clusters</th>\n",
       "      <th>Silhouette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>130</td>\n",
       "      <td>29</td>\n",
       "      <td>0.384419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>130</td>\n",
       "      <td>28</td>\n",
       "      <td>0.383546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>110</td>\n",
       "      <td>29</td>\n",
       "      <td>0.383175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>110</td>\n",
       "      <td>28</td>\n",
       "      <td>0.381151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>110</td>\n",
       "      <td>27</td>\n",
       "      <td>0.381061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>130</td>\n",
       "      <td>27</td>\n",
       "      <td>0.378714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>110</td>\n",
       "      <td>26</td>\n",
       "      <td>0.375213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>120</td>\n",
       "      <td>29</td>\n",
       "      <td>0.374678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.374640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>130</td>\n",
       "      <td>25</td>\n",
       "      <td>0.374050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>100</td>\n",
       "      <td>29</td>\n",
       "      <td>0.373632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>130</td>\n",
       "      <td>26</td>\n",
       "      <td>0.372910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>130</td>\n",
       "      <td>24</td>\n",
       "      <td>0.372277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>110</td>\n",
       "      <td>25</td>\n",
       "      <td>0.371491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "      <td>0.370543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>0.369713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.369420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0.369356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>120</td>\n",
       "      <td>28</td>\n",
       "      <td>0.368805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.368707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>130</td>\n",
       "      <td>23</td>\n",
       "      <td>0.368650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>120</td>\n",
       "      <td>27</td>\n",
       "      <td>0.368436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.368149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>160</td>\n",
       "      <td>29</td>\n",
       "      <td>0.366745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>100</td>\n",
       "      <td>27</td>\n",
       "      <td>0.366658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.365981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>190</td>\n",
       "      <td>28</td>\n",
       "      <td>0.365896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.365849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.365537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.365178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>110</td>\n",
       "      <td>23</td>\n",
       "      <td>0.364951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>130</td>\n",
       "      <td>22</td>\n",
       "      <td>0.364911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>80</td>\n",
       "      <td>29</td>\n",
       "      <td>0.364816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>190</td>\n",
       "      <td>27</td>\n",
       "      <td>0.364773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>0.364287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>160</td>\n",
       "      <td>27</td>\n",
       "      <td>0.363663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.363276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>80</td>\n",
       "      <td>23</td>\n",
       "      <td>0.363181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>190</td>\n",
       "      <td>29</td>\n",
       "      <td>0.362533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>0.362435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>100</td>\n",
       "      <td>26</td>\n",
       "      <td>0.362192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>80</td>\n",
       "      <td>26</td>\n",
       "      <td>0.362073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>120</td>\n",
       "      <td>26</td>\n",
       "      <td>0.362015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>160</td>\n",
       "      <td>25</td>\n",
       "      <td>0.361561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>160</td>\n",
       "      <td>19</td>\n",
       "      <td>0.361475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>0.361431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>110</td>\n",
       "      <td>22</td>\n",
       "      <td>0.361256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>160</td>\n",
       "      <td>11</td>\n",
       "      <td>0.361205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>170</td>\n",
       "      <td>29</td>\n",
       "      <td>0.360628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>0.360607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>120</td>\n",
       "      <td>13</td>\n",
       "      <td>0.360296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.359787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>160</td>\n",
       "      <td>20</td>\n",
       "      <td>0.359752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>80</td>\n",
       "      <td>28</td>\n",
       "      <td>0.359737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>160</td>\n",
       "      <td>28</td>\n",
       "      <td>0.359722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>80</td>\n",
       "      <td>22</td>\n",
       "      <td>0.359664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>160</td>\n",
       "      <td>26</td>\n",
       "      <td>0.359561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>120</td>\n",
       "      <td>25</td>\n",
       "      <td>0.359520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>160</td>\n",
       "      <td>23</td>\n",
       "      <td>0.359439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>0.359282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>140</td>\n",
       "      <td>26</td>\n",
       "      <td>0.358996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>160</td>\n",
       "      <td>24</td>\n",
       "      <td>0.358724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>170</td>\n",
       "      <td>22</td>\n",
       "      <td>0.358692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>50</td>\n",
       "      <td>27</td>\n",
       "      <td>0.358171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>170</td>\n",
       "      <td>27</td>\n",
       "      <td>0.358098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>190</td>\n",
       "      <td>25</td>\n",
       "      <td>0.358095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>80</td>\n",
       "      <td>25</td>\n",
       "      <td>0.358011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>190</td>\n",
       "      <td>26</td>\n",
       "      <td>0.357987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>0.357929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>170</td>\n",
       "      <td>28</td>\n",
       "      <td>0.357913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>150</td>\n",
       "      <td>29</td>\n",
       "      <td>0.357763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>120</td>\n",
       "      <td>24</td>\n",
       "      <td>0.357625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "      <td>0.357610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>0.357593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>0.357491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>160</td>\n",
       "      <td>10</td>\n",
       "      <td>0.357404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>160</td>\n",
       "      <td>22</td>\n",
       "      <td>0.357306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>0.357297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>170</td>\n",
       "      <td>20</td>\n",
       "      <td>0.357241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>140</td>\n",
       "      <td>27</td>\n",
       "      <td>0.357095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>0.356963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>150</td>\n",
       "      <td>28</td>\n",
       "      <td>0.356686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>170</td>\n",
       "      <td>21</td>\n",
       "      <td>0.356642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>0.356481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>170</td>\n",
       "      <td>26</td>\n",
       "      <td>0.356288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>160</td>\n",
       "      <td>21</td>\n",
       "      <td>0.356136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>80</td>\n",
       "      <td>24</td>\n",
       "      <td>0.355901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>160</td>\n",
       "      <td>12</td>\n",
       "      <td>0.355895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>0.355589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>50</td>\n",
       "      <td>29</td>\n",
       "      <td>0.355407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>180</td>\n",
       "      <td>10</td>\n",
       "      <td>0.355353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>130</td>\n",
       "      <td>21</td>\n",
       "      <td>0.355123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>190</td>\n",
       "      <td>24</td>\n",
       "      <td>0.355036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>80</td>\n",
       "      <td>21</td>\n",
       "      <td>0.355018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>190</td>\n",
       "      <td>22</td>\n",
       "      <td>0.354874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>0.354775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>170</td>\n",
       "      <td>25</td>\n",
       "      <td>0.354290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>110</td>\n",
       "      <td>21</td>\n",
       "      <td>0.354166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.354104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>120</td>\n",
       "      <td>23</td>\n",
       "      <td>0.354014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>170</td>\n",
       "      <td>3</td>\n",
       "      <td>0.353814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>120</td>\n",
       "      <td>10</td>\n",
       "      <td>0.353717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>150</td>\n",
       "      <td>27</td>\n",
       "      <td>0.353672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0.353643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>200</td>\n",
       "      <td>29</td>\n",
       "      <td>0.353554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>140</td>\n",
       "      <td>28</td>\n",
       "      <td>0.353213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>0.353038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>120</td>\n",
       "      <td>21</td>\n",
       "      <td>0.353024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>120</td>\n",
       "      <td>11</td>\n",
       "      <td>0.352977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>0.352770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.352700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>0.352675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>120</td>\n",
       "      <td>18</td>\n",
       "      <td>0.352663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>180</td>\n",
       "      <td>22</td>\n",
       "      <td>0.352635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>140</td>\n",
       "      <td>25</td>\n",
       "      <td>0.352625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>170</td>\n",
       "      <td>24</td>\n",
       "      <td>0.352591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>170</td>\n",
       "      <td>19</td>\n",
       "      <td>0.352381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>70</td>\n",
       "      <td>20</td>\n",
       "      <td>0.352379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>80</td>\n",
       "      <td>18</td>\n",
       "      <td>0.352302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>160</td>\n",
       "      <td>8</td>\n",
       "      <td>0.352258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>150</td>\n",
       "      <td>11</td>\n",
       "      <td>0.352245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>0.352131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>120</td>\n",
       "      <td>17</td>\n",
       "      <td>0.352081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0.351977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>120</td>\n",
       "      <td>22</td>\n",
       "      <td>0.351463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>0.351403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>0.351376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>170</td>\n",
       "      <td>23</td>\n",
       "      <td>0.351350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>80</td>\n",
       "      <td>19</td>\n",
       "      <td>0.351334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>100</td>\n",
       "      <td>22</td>\n",
       "      <td>0.351279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>120</td>\n",
       "      <td>19</td>\n",
       "      <td>0.351153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.351106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>0.350893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>120</td>\n",
       "      <td>16</td>\n",
       "      <td>0.350843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>190</td>\n",
       "      <td>21</td>\n",
       "      <td>0.350658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>70</td>\n",
       "      <td>19</td>\n",
       "      <td>0.350462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>110</td>\n",
       "      <td>11</td>\n",
       "      <td>0.350327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>120</td>\n",
       "      <td>20</td>\n",
       "      <td>0.350324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>190</td>\n",
       "      <td>23</td>\n",
       "      <td>0.350253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>130</td>\n",
       "      <td>20</td>\n",
       "      <td>0.350253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>70</td>\n",
       "      <td>21</td>\n",
       "      <td>0.350233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>140</td>\n",
       "      <td>29</td>\n",
       "      <td>0.350208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>0.349842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>150</td>\n",
       "      <td>25</td>\n",
       "      <td>0.349587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>180</td>\n",
       "      <td>21</td>\n",
       "      <td>0.349541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>180</td>\n",
       "      <td>23</td>\n",
       "      <td>0.349525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>110</td>\n",
       "      <td>8</td>\n",
       "      <td>0.349480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>0.349352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.349321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>110</td>\n",
       "      <td>9</td>\n",
       "      <td>0.349265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0.348966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>50</td>\n",
       "      <td>24</td>\n",
       "      <td>0.348928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.348541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>160</td>\n",
       "      <td>9</td>\n",
       "      <td>0.348441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>70</td>\n",
       "      <td>29</td>\n",
       "      <td>0.348296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>180</td>\n",
       "      <td>20</td>\n",
       "      <td>0.348224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>130</td>\n",
       "      <td>19</td>\n",
       "      <td>0.348084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>0.348006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>50</td>\n",
       "      <td>28</td>\n",
       "      <td>0.347502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>110</td>\n",
       "      <td>20</td>\n",
       "      <td>0.347447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>200</td>\n",
       "      <td>15</td>\n",
       "      <td>0.347199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>0.346740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>0.346712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>160</td>\n",
       "      <td>18</td>\n",
       "      <td>0.346350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>70</td>\n",
       "      <td>28</td>\n",
       "      <td>0.346291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>190</td>\n",
       "      <td>20</td>\n",
       "      <td>0.346236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>200</td>\n",
       "      <td>28</td>\n",
       "      <td>0.346142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>160</td>\n",
       "      <td>4</td>\n",
       "      <td>0.345979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>0.345801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.345685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>130</td>\n",
       "      <td>17</td>\n",
       "      <td>0.345603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>140</td>\n",
       "      <td>24</td>\n",
       "      <td>0.345561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.345427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>140</td>\n",
       "      <td>12</td>\n",
       "      <td>0.345307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>0.345152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>150</td>\n",
       "      <td>26</td>\n",
       "      <td>0.345077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.345045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>120</td>\n",
       "      <td>8</td>\n",
       "      <td>0.345044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>0.344935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TSNE perplexity  Num. clusters  Silhouette\n",
       "363              130             29    0.384419\n",
       "362              130             28    0.383546\n",
       "307              110             29    0.383175\n",
       "306              110             28    0.381151\n",
       "305              110             27    0.381061\n",
       "361              130             27    0.378714\n",
       "304              110             26    0.375213\n",
       "335              120             29    0.374678\n",
       "0                 10              2    0.374640\n",
       "359              130             25    0.374050\n",
       "279              100             29    0.373632\n",
       "360              130             26    0.372910\n",
       "358              130             24    0.372277\n",
       "303              110             25    0.371491\n",
       "302              110             24    0.370543\n",
       "278              100             28    0.369713\n",
       "12                10             14    0.369420\n",
       "11                10             13    0.369356\n",
       "334              120             28    0.368805\n",
       "13                10             15    0.368707\n",
       "357              130             23    0.368650\n",
       "333              120             27    0.368436\n",
       "15                10             17    0.368149\n",
       "447              160             29    0.366745\n",
       "277              100             27    0.366658\n",
       "8                 10             10    0.365981\n",
       "530              190             28    0.365896\n",
       "24                10             26    0.365849\n",
       "7                 10              9    0.365537\n",
       "14                10             16    0.365178\n",
       "301              110             23    0.364951\n",
       "356              130             22    0.364911\n",
       "16                10             18    0.364900\n",
       "223               80             29    0.364816\n",
       "529              190             27    0.364773\n",
       "54                20             28    0.364287\n",
       "445              160             27    0.363663\n",
       "10                10             12    0.363276\n",
       "217               80             23    0.363181\n",
       "531              190             29    0.362533\n",
       "27                10             29    0.362435\n",
       "276              100             26    0.362192\n",
       "220               80             26    0.362073\n",
       "332              120             26    0.362015\n",
       "443              160             25    0.361561\n",
       "437              160             19    0.361475\n",
       "53                20             27    0.361431\n",
       "300              110             22    0.361256\n",
       "429              160             11    0.361205\n",
       "475              170             29    0.360628\n",
       "318              120             12    0.360607\n",
       "319              120             13    0.360296\n",
       "275              100             25    0.359787\n",
       "438              160             20    0.359752\n",
       "222               80             28    0.359737\n",
       "446              160             28    0.359722\n",
       "216               80             22    0.359664\n",
       "444              160             26    0.359561\n",
       "331              120             25    0.359520\n",
       "441              160             23    0.359439\n",
       "26                10             28    0.359282\n",
       "388              140             26    0.358996\n",
       "442              160             24    0.358724\n",
       "468              170             22    0.358692\n",
       "137               50             27    0.358171\n",
       "473              170             27    0.358098\n",
       "527              190             25    0.358095\n",
       "219               80             25    0.358011\n",
       "528              190             26    0.357987\n",
       "55                20             29    0.357929\n",
       "474              170             28    0.357913\n",
       "419              150             29    0.357763\n",
       "330              120             24    0.357625\n",
       "221               80             27    0.357610\n",
       "52                20             26    0.357593\n",
       "136               50             26    0.357491\n",
       "428              160             10    0.357404\n",
       "440              160             22    0.357306\n",
       "23                10             25    0.357297\n",
       "466              170             20    0.357241\n",
       "389              140             27    0.357095\n",
       "22                10             24    0.356963\n",
       "418              150             28    0.356686\n",
       "467              170             21    0.356642\n",
       "20                10             22    0.356481\n",
       "472              170             26    0.356288\n",
       "439              160             21    0.356136\n",
       "218               80             24    0.355901\n",
       "430              160             12    0.355895\n",
       "51                20             25    0.355589\n",
       "139               50             29    0.355407\n",
       "484              180             10    0.355353\n",
       "355              130             21    0.355123\n",
       "526              190             24    0.355036\n",
       "215               80             21    0.355018\n",
       "524              190             22    0.354874\n",
       "19                10             21    0.354775\n",
       "471              170             25    0.354290\n",
       "299              110             21    0.354166\n",
       "9                 10             11    0.354104\n",
       "329              120             23    0.354014\n",
       "449              170              3    0.353814\n",
       "316              120             10    0.353717\n",
       "417              150             27    0.353672\n",
       "25                10             27    0.353643\n",
       "559              200             29    0.353554\n",
       "390              140             28    0.353213\n",
       "188               70             22    0.353038\n",
       "327              120             21    0.353024\n",
       "317              120             11    0.352977\n",
       "274              100             24    0.352770\n",
       "17                10             19    0.352700\n",
       "450              170              4    0.352675\n",
       "324              120             18    0.352663\n",
       "496              180             22    0.352635\n",
       "387              140             25    0.352625\n",
       "470              170             24    0.352591\n",
       "465              170             19    0.352381\n",
       "186               70             20    0.352379\n",
       "212               80             18    0.352302\n",
       "426              160              8    0.352258\n",
       "401              150             11    0.352245\n",
       "315              120              9    0.352131\n",
       "323              120             17    0.352081\n",
       "477              180              3    0.351977\n",
       "328              120             22    0.351463\n",
       "135               50             25    0.351403\n",
       "50                20             24    0.351376\n",
       "469              170             23    0.351350\n",
       "213               80             19    0.351334\n",
       "272              100             22    0.351279\n",
       "325              120             19    0.351153\n",
       "1                 10              3    0.351106\n",
       "21                10             23    0.350893\n",
       "322              120             16    0.350843\n",
       "523              190             21    0.350658\n",
       "185               70             19    0.350462\n",
       "289              110             11    0.350327\n",
       "326              120             20    0.350324\n",
       "525              190             23    0.350253\n",
       "354              130             20    0.350253\n",
       "187               70             21    0.350233\n",
       "391              140             29    0.350208\n",
       "39                20             13    0.349842\n",
       "415              150             25    0.349587\n",
       "495              180             21    0.349541\n",
       "497              180             23    0.349525\n",
       "286              110              8    0.349480\n",
       "352              130             18    0.349352\n",
       "273              100             23    0.349321\n",
       "287              110              9    0.349265\n",
       "38                20             12    0.348966\n",
       "134               50             24    0.348928\n",
       "49                20             23    0.348541\n",
       "427              160              9    0.348441\n",
       "195               70             29    0.348296\n",
       "494              180             20    0.348224\n",
       "353              130             19    0.348084\n",
       "214               80             20    0.348006\n",
       "138               50             28    0.347502\n",
       "298              110             20    0.347447\n",
       "545              200             15    0.347199\n",
       "40                20             14    0.346740\n",
       "365              140              3    0.346712\n",
       "436              160             18    0.346350\n",
       "194               70             28    0.346291\n",
       "522              190             20    0.346236\n",
       "558              200             28    0.346142\n",
       "422              160              4    0.345979\n",
       "400              150             10    0.345801\n",
       "60                30              6    0.345685\n",
       "351              130             17    0.345603\n",
       "386              140             24    0.345561\n",
       "271              100             21    0.345427\n",
       "374              140             12    0.345307\n",
       "48                20             22    0.345152\n",
       "416              150             26    0.345077\n",
       "18                10             20    0.345045\n",
       "314              120              8    0.345044\n",
       "192               70             26    0.344935"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           8
          ],
          [
           15
          ],
          [
           5
          ],
          [
           20
          ],
          [
           21
          ],
          [
           6
          ],
          [
           26
          ],
          [
           2
          ],
          [
           1
          ],
          [
           1
          ],
          [
           14
          ],
          [
           1
          ],
          [
           7
          ],
          [
           9
          ],
          [
           8
          ],
          [
           4
          ],
          [
           15
          ],
          [
           16
          ],
          [
           20
          ],
          [
           1
          ],
          [
           1
          ],
          [
           6
          ],
          [
           19
          ],
          [
           13
          ],
          [
           14
          ],
          [
           5
          ],
          [
           20
          ],
          [
           5
          ],
          [
           7
          ],
          [
           5
          ],
          [
           5
          ],
          [
           17
          ],
          [
           3
          ],
          [
           16
          ],
          [
           4
          ],
          [
           28
          ],
          [
           9
          ],
          [
           7
          ],
          [
           24
          ],
          [
           2
          ],
          [
           21
          ],
          [
           28
          ],
          [
           9
          ],
          [
           15
          ],
          [
           14
          ],
          [
           26
          ],
          [
           16
          ],
          [
           17
          ],
          [
           18
          ],
          [
           7
          ],
          [
           21
          ],
          [
           4
          ],
          [
           12
          ],
          [
           12
          ],
          [
           3
          ],
          [
           6
          ],
          [
           1
          ],
          [
           16
          ],
          [
           0
          ],
          [
           1
          ],
          [
           10
          ],
          [
           13
          ],
          [
           3
          ],
          [
           7
          ],
          [
           9
          ],
          [
           5
          ],
          [
           24
          ],
          [
           24
          ],
          [
           22
          ],
          [
           28
          ],
          [
           21
          ],
          [
           8
          ],
          [
           21
          ],
          [
           19
          ],
          [
           14
          ],
          [
           8
          ],
          [
           20
          ],
          [
           4
          ],
          [
           19
          ],
          [
           17
          ],
          [
           5
          ],
          [
           14
          ],
          [
           22
          ],
          [
           22
          ],
          [
           26
          ],
          [
           1
          ],
          [
           15
          ],
          [
           7
          ],
          [
           5
          ],
          [
           8
          ],
          [
           15
          ],
          [
           21
          ],
          [
           28
          ],
          [
           16
          ],
          [
           2
          ],
          [
           5
          ],
          [
           12
          ],
          [
           20
          ],
          [
           7
          ],
          [
           11
          ],
          [
           8
          ],
          [
           17
          ],
          [
           14
          ],
          [
           28
          ],
          [
           20
          ],
          [
           2
          ],
          [
           6
          ],
          [
           28
          ],
          [
           8
          ],
          [
           15
          ],
          [
           3
          ],
          [
           8
          ],
          [
           10
          ],
          [
           10
          ],
          [
           9
          ],
          [
           10
          ],
          [
           5
          ],
          [
           21
          ],
          [
           26
          ],
          [
           7
          ],
          [
           21
          ],
          [
           10
          ],
          [
           12
          ],
          [
           16
          ],
          [
           3
          ],
          [
           7
          ],
          [
           12
          ],
          [
           21
          ],
          [
           12
          ],
          [
           9
          ],
          [
           0
          ],
          [
           16
          ],
          [
           2
          ],
          [
           5
          ],
          [
           24
          ],
          [
           23
          ],
          [
           8
          ],
          [
           2
          ],
          [
           17
          ],
          [
           26
          ],
          [
           16
          ],
          [
           4
          ],
          [
           15
          ],
          [
           10
          ],
          [
           21
          ],
          [
           21
          ],
          [
           13
          ],
          [
           16
          ],
          [
           20
          ],
          [
           7
          ],
          [
           27
          ],
          [
           17
          ],
          [
           8
          ],
          [
           20
          ],
          [
           0
          ],
          [
           15
          ],
          [
           0
          ],
          [
           5
          ],
          [
           12
          ],
          [
           0
          ],
          [
           24
          ],
          [
           15
          ],
          [
           8
          ],
          [
           20
          ],
          [
           11
          ],
          [
           25
          ],
          [
           13
          ],
          [
           21
          ],
          [
           3
          ],
          [
           11
          ],
          [
           26
          ],
          [
           8
          ],
          [
           4
          ],
          [
           2
          ],
          [
           2
          ],
          [
           17
          ],
          [
           24
          ],
          [
           0
          ],
          [
           12
          ],
          [
           11
          ],
          [
           20
          ],
          [
           26
          ],
          [
           19
          ],
          [
           26
          ],
          [
           0
          ],
          [
           28
          ],
          [
           1
          ],
          [
           15
          ],
          [
           15
          ],
          [
           25
          ],
          [
           1
          ],
          [
           2
          ],
          [
           8
          ],
          [
           17
          ],
          [
           3
          ],
          [
           26
          ],
          [
           7
          ],
          [
           4
          ],
          [
           15
          ],
          [
           1
          ],
          [
           6
          ],
          [
           18
          ],
          [
           1
          ],
          [
           10
          ],
          [
           13
          ],
          [
           7
          ],
          [
           15
          ],
          [
           26
          ],
          [
           3
          ],
          [
           1
          ],
          [
           18
          ],
          [
           20
          ],
          [
           22
          ],
          [
           24
          ],
          [
           27
          ],
          [
           17
          ],
          [
           20
          ],
          [
           24
          ],
          [
           18
          ],
          [
           27
          ],
          [
           17
          ],
          [
           0
          ],
          [
           17
          ],
          [
           9
          ],
          [
           24
          ],
          [
           0
          ],
          [
           10
          ],
          [
           1
          ],
          [
           1
          ],
          [
           0
          ],
          [
           5
          ],
          [
           28
          ],
          [
           13
          ],
          [
           13
          ],
          [
           20
          ],
          [
           26
          ],
          [
           16
          ],
          [
           15
          ],
          [
           10
          ],
          [
           0
          ],
          [
           13
          ],
          [
           20
          ],
          [
           6
          ],
          [
           3
          ],
          [
           23
          ],
          [
           28
          ],
          [
           10
          ],
          [
           13
          ],
          [
           9
          ],
          [
           9
          ],
          [
           20
          ],
          [
           12
          ],
          [
           7
          ],
          [
           12
          ],
          [
           14
          ],
          [
           27
          ],
          [
           20
          ],
          [
           24
          ],
          [
           17
          ],
          [
           0
          ],
          [
           12
          ],
          [
           27
          ],
          [
           3
          ],
          [
           2
          ],
          [
           2
          ],
          [
           14
          ],
          [
           9
          ],
          [
           10
          ],
          [
           7
          ],
          [
           9
          ],
          [
           3
          ],
          [
           8
          ],
          [
           7
          ],
          [
           1
          ],
          [
           14
          ],
          [
           1
          ],
          [
           20
          ],
          [
           17
          ],
          [
           0
          ],
          [
           2
          ],
          [
           7
          ],
          [
           14
          ],
          [
           15
          ],
          [
           10
          ],
          [
           28
          ],
          [
           12
          ],
          [
           9
          ],
          [
           9
          ],
          [
           4
          ],
          [
           5
          ],
          [
           10
          ],
          [
           17
          ],
          [
           25
          ],
          [
           23
          ],
          [
           12
          ],
          [
           3
          ],
          [
           9
          ],
          [
           13
          ],
          [
           17
          ],
          [
           24
          ],
          [
           11
          ],
          [
           17
          ],
          [
           9
          ],
          [
           25
          ],
          [
           2
          ],
          [
           11
          ],
          [
           6
          ],
          [
           18
          ],
          [
           18
          ],
          [
           20
          ],
          [
           2
          ],
          [
           17
          ],
          [
           18
          ],
          [
           0
          ],
          [
           1
          ],
          [
           24
          ],
          [
           19
          ],
          [
           11
          ],
          [
           20
          ],
          [
           17
          ]
         ],
         "hovertemplate": "<b>%{hovertext}</b><br><br>x_tsne=%{x}<br>y_tsne=%{y}<br>label=%{customdata[0]}<br>color=%{marker.color}<extra></extra>",
         "hovertext": [
          "energy-efficient and privacy-aware social distance monitoring with low-resolution infrared sensors and adaptive inference",
          "moduleformer: learning modular large language models from uncurated data",
          "patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks",
          "soft merging of experts with adaptive routing",
          "towards anytime classification in early-exit architectures by enforcing conditional monotonicity",
          "csi-based efficient self-quarantine monitoring system using branchy convolution neural network",
          "marsellus: a heterogeneous risc-v ai-iot end-node soc with 2-to-8b dnn acceleration and 30%-boost adaptive body biasing",
          "towards convergence rates for parameter estimation in gaussian-gated mixture of experts",
          "hypere2vid: improving event-based video reconstruction via hypernetworks",
          "transforming visual scene graphs to image captions",
          "big-little adaptive neural networks on low-power near-subthreshold processors",
          "long-distance gesture recognition using dynamic neural networks",
          "atheena: a toolflow for hardware early-exit network automation",
          "gradmdm: adversarial attack on dynamic networks",
          "dynamicdet: a unified dynamic architecture for object detection",
          "graph mixture of experts: learning on large-scale graphs with explicit diversity modeling",
          "long-tailed visual recognition via self-heterogeneous integration with knowledge excavation",
          "seenn: towards temporal spiking early-exit neural networks",
          "evolving artificial neural networks to imitate human behaviour in shinobi iii : return of the ninja master",
          "re-iqa: unsupervised learning for image quality assessment in the wild",
          "a small-scale switch transformer and nlp-based model for clinical narratives classification",
          "dynamic healthcare embeddings for improving patient care",
          "improving deep attractor network by bgru and gmm for speech separation",
          "memorization capacity of neural networks with conditional computation",
          "gated compression layers for efficient always-on models",
          "simultaneous action recognition and human whole-body motion and dynamics prediction from wearable sensors",
          "dynamic neural network for multi-task learning searching across diverse network topologies",
          "a hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training",
          "hierarchical training of deep neural networks using early exiting",
          "improving expert specialization in mixture of experts",
          "ta-moe: topology-aware large scale mixture-of-expert training",
          "fixing overconfidence in dynamic neural networks",
          "gaussian process-gated hierarchical mixtures of experts",
          "lgvit: dynamic early exiting for accelerating vision transformer",
          "ed-batch: efficient automatic batching of dynamic neural networks via learned finite state machines",
          "enhancing once-for-all: a study on parallel blocks, skip connections and early exits",
          "fast, differentiable and sparse top-k: a convex analysis perspective",
          "anticipate, ensemble and prune: improving convolutional neural networks via aggregated early exits",
          "neural abstractions",
          "adaensemble: learning adaptively sparse structured ensemble network for click-through rate prediction",
          "eenet: learning to early exit for adaptive inference",
          "post-train adaptive u-net for image segmentation",
          "adaptive neural networks using residual fitting",
          "heterogeneous domain adaptation and equipment matching: dann-based alignment with cyclic supervision (dbacs)",
          "monadic deep learning",
          "accuracy-guaranteed collaborative dnn inference in industrial iot via deep reinforcement learning",
          "quicknets: saving training and preventing overconfidence in early-exit neural architectures",
          "sparse upcycling: training mixture-of-experts from dense checkpoints",
          "adaptive neural network backstepping control method for aerial manipulator based on variable inertia parameter modeling",
          "hadas: hardware-aware dynamic neural architecture search for edge performance scaling",
          "understanding the robustness of multi-exit models under common corruptions",
          "boosted dynamic neural networks",
          "cfnet: conditional filter learning with dynamic noise estimation for real image denoising",
          "spatial mixture-of-experts",
          "physics-informed koopman network",
          "alioth: a machine learning based interference-aware performance monitor for multi-tenancy applications in public cloud",
          "dynamic-pix2pix: noise injected cgan for modeling input and target domain joint distributions with limited training data",
          "cold start streaming learning for deep networks",
          "safe real-world autonomous driving by learning to predict and plan with a mixture of experts",
          "tiny-attention adapter: contexts are more important than the number of parameters",
          "parallel gated neural network with attention mechanism for speech enhancement",
          "adaptive neural network ensemble using frequency distribution",
          "on the adversarial robustness of mixture of experts",
          "a novel membership inference attack against dynamic neural networks by utilizing policy networks information",
          "dynamics-aware adversarial attack of adaptive neural networks",
          "feamoe: fair, explainable and adaptive mixture of experts",
          "trainability, expressivity and interpretability in gated neural odes",
          "probabilistic partition of unity networks for high-dimensional regression problems",
          "sparsity-constrained optimal transport",
          "tuning of mixture-of-experts mixed-precision neural networks",
          "fluid batching: exit-aware preemptive serving of early-exit neural networks on edge npus",
          "sda-\nx\nnet: selective depth attention networks for adaptive multi-scale feature representation",
          "improving the performance of dnn-based software services using automated layer caching",
          "a new hazard event classification model via deep learning and multifractal",
          "a survey of neural trees",
          "human activity recognition on microcontrollers with quantized and adaptive deep neural networks",
          "efficient sparsely activated transformers",
          "dycl: dynamic neural network compilation via program rewriting and graph optimization",
          "admoe: anomaly detection with mixture-of-experts from noisy labels",
          "a theoretical view on sparsely activated networks",
          "towards understanding mixture of experts in deep learning",
          "on-demand resource management for 6g wireless networks using knowledge-assisted dynamic neural networks",
          "test-time adaptation via conjugate pseudo-labels",
          "eve: environmental adaptive neural network models for low-power energy harvesting system",
          "t-recx: tiny-resource efficient convolutional neural networks with early-exit",
          "omni-seg: a scale-aware dynamic network for renal pathological image segmentation",
          "scai: a spectral data classification framework with adaptive inference for the iot platform",
          "binary early-exit network for adaptive inference on low-resource devices",
          "the future of human-centric explainable artificial intelligence (xai) is not post-hoc explanations",
          "satbench: benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks",
          "fault-tolerant collaborative inference through the edge-prune framework",
          "resource-constrained edge ai with early exit prediction",
          "exploring the intersection between neural architecture search and continual learning",
          "continuer: maintaining distributed dnn services during edge failures",
          "uni-perceiver-moe: learning sparse generalist models with conditional moes",
          "adaptive neural network-based unscented kalman filter for robust pose tracking of noncooperative spacecraft",
          "interpretable mixture of experts",
          "temporal domain generalization with drift-aware dynamic neural networks",
          "optimizing mixture of experts using dynamic recompilations",
          "stock price prediction using dynamic neural networks",
          "test-time adaptable neural networks for robust medical image segmentation",
          "a survey on dynamic neural networks for natural language processing",
          "ereba: black-box energy testing of adaptive neural networks",
          "physics-guided problem decomposition for scaling deep learning of high-dimensional eigen-solvers: the case of schrödinger's equation",
          "a field of experts prior for adapting neural networks at test time",
          "towards lightweight neural animation : exploration of neural network pruning in mixture of experts-based animation models",
          "mobilefaceswap: a lightweight framework for video face swapping",
          "an adaptive device-edge co-inference framework based on soft actor-critic",
          "feature matching as improved transfer learning technique for wearable eeg",
          "heterogeneous transformer: a scale adaptable neural network architecture for device activity detection",
          "learning over all stabilizing nonlinear controllers for a partially-observed linear system",
          "towards efficient single image dehazing and desnowing",
          "a mixture of expert based deep neural network for improved asr",
          "privacy attacks for automatic speech recognition acoustic models in a federated learning framework",
          "polynomial-spline neural networks with exact integrals",
          "private language model adaptation for speech recognition",
          "sparse moes meet efficient ensembles",
          "consistency training of multi-exit architectures for sensor data",
          "dystyle: dynamic neural network for multi-attribute-conditioned style editing",
          "dynamic neural network architectural and topological adaptation and related methods -- a survey",
          "early-exit deep neural networks for distorted images: providing an efficient edge offloading",
          "airex: neural network-based approach for air quality inference in unmonitored cities",
          "table-based fact verification with self-adaptive mixture of experts",
          "eeea-net: an early exit evolutionary neural architecture search",
          "forecasting the outcome of spintronic experiments with neural ordinary differential equations",
          "few-shot and continual learning with attentive independent mechanisms",
          "federated mixture of experts",
          "multi-exit vision transformer for dynamic inference",
          "mixtures of deep neural experts for automated speech scoring",
          "better training using weight-constrained stochastic dynamics",
          "machine learning methods for postprocessing ensemble forecasts of wind gusts: a systematic comparison",
          "zero time waste: recycling predictions in early exit neural networks",
          "dselect-k: differentiable selection in the mixture of experts with applications to multi-task learning",
          "moebert: from bert to mixture-of-experts via importance-guided adaptation",
          "student performance prediction using dynamic neural models",
          "mixture of elm based experts with trainable gating network",
          "show why the answer is correct! towards explainable ai using compositional temporal attention",
          "kdexplainer: a task-oriented attention model for explaining knowledge distillation",
          "basisnet: two-stage model synthesis for efficient inference",
          "ecnns: ensemble learning methods for improving planar grasp quality estimation",
          "improving the accuracy of early exits in multi-exit architectures via curriculum learning",
          "dyno: dynamic onloading of deep neural networks from cloud to device",
          "contextual hypernetworks for novel feature adaptation",
          "deep dynamic neural network to trade-off between accuracy and diversity in a news recommender system",
          "resource allocation for multiuser edge inference with batching and early exiting (extended version)",
          "split computing and early exiting for deep learning applications: survey and research challenges",
          "task-adaptive neural network search with meta-contrastive learning",
          "e\n2\ncm: early exit via class means for efficient supervised and unsupervised learning",
          "deep learning with a classifier system: initial results",
          "embedded knowledge distillation in depth-level dynamic neural network",
          "siamese labels auxiliary learning",
          "dynamic neural networks: a survey",
          "learning task-oriented communication for edge inference: an information bottleneck approach",
          "a reproducibility study of \"augmenting genetic algorithms with deep neural networks for exploring the chemical space\"",
          "precise motion control of wafer stages via adaptive neural network and fractional-order super-twisting algorithm",
          "understanding the influence of receptive field and network complexity in neural-network-guided tem image analysis",
          "a novel cluster classify regress model predictive controller formulation; ccr-mpc",
          "gated ensemble of spatio-temporal mixture of experts for multi-task learning in ride-hailing system",
          "self-supervised multimodal domino: in search of biomarkers for alzheimer's disease",
          "multi-expert learning of adaptive legged locomotion",
          "diffprune: neural network pruning with deterministic approximate binary gates and\nl\n0\nregularization",
          "anytime prediction as a model of human reaction time",
          "dadnn: multi-scene ctr prediction via domain-aware deep neural network",
          "nested mixture of experts: cooperative and competitive learning of hybrid dynamical system",
          "discovery of the hidden state in ionic models using a domain-specific recurrent neural network",
          "dynamically throttleable neural networks (tnn)",
          "adaptive neural network-based ofdm receivers",
          "branchy-gnn: a device-edge co-inference framework for efficient point cloud processing",
          "towards a universal gating network for mixtures of experts",
          "adapting neural networks for uplift models",
          "calibration-aided edge inference offloading via adaptive model partitioning of deep neural networks",
          "glance and focus: a dynamic approach to reducing spatial redundancy in image classification",
          "revisiting batch normalization for training low-latency deep spiking neural networks from scratch",
          "identification of probability weighted arx models with arbitrary domains",
          "flight-connection prediction for airline crew scheduling to construct initial clusters for or optimizer",
          "neurocoder: learning general-purpose computation using stored neural programs",
          "aidx: adaptive inference scheme to mitigate state-drift in memristive vmm accelerators",
          "flash: fast and light motion prediction for autonomous driving with bayesian inverse planning and learned motion profiles",
          "anomaly detection by recombining gated unsupervised experts",
          "adaptive neural network-based approximation to accelerate eulerian fluid simulation",
          "making neural networks interpretable with attribution: application to implicit signals prediction",
          "spinn: synergistic progressive inference of neural networks over device and cloud",
          "mixcaps: a capsule network-based mixture of experts for lung nodule malignancy prediction",
          "hapi: hardware-aware progressive inference",
          "velocity regulation of 3d bipedal walking robots with uncertain dynamics through adaptive neural network controller",
          "wrapnet: neural net inference with ultra-low-resolution arithmetic",
          "learning to compose hypercolumns for visual correspondence",
          "learning to learn parameterized classification networks for scalable input images",
          "summareranker: a multi-task mixture-of-experts re-ranking framework for abstractive summarization",
          "gshard: scaling giant models with conditional computation and automatic sharding",
          "early exit or not: resource-efficient blind quality enhancement for compressed images",
          "fast deep mixtures of gaussian process experts",
          "elf: an early-exiting framework for long-tailed classification",
          "nimble: efficiently compiling dynamic neural networks for model inference",
          "surprisal-triggered conditional computation with neural networks",
          "conditionally deep hybrid neural networks across edge and cloud",
          "faster depth-adaptive transformers",
          "why should we add early exits to neural networks?",
          "the right tool for the job: matching model and instance complexities",
          "dynamic instance domain adaptation",
          "deepcare: a deep dynamic memory model for predictive medicine",
          "learning charme models with neural networks",
          "gla in mediaeval 2018 emotional impact of movies task",
          "improving distant supervised relation extraction by dynamic neural network",
          "hierarchical mixtures of generators for adversarial learning",
          "edge ai: on-demand accelerating deep neural network inference via edge computing",
          "cross-modal subspace learning with scheduled adaptive margin constraints",
          "learning sparse mixture of experts for visual question answering",
          "mpc-net: a first principles guided policy search",
          "expert sample consensus applied to camera re-localization",
          "an adaptive architecture for portability of greenhouse models",
          "deep adaptive inference networks for single image super-resolution",
          "an empirical study of batch normalization and group normalization in conditional computation",
          "controlling model complexity in probabilistic model-based dynamic optimization of neural network structures",
          "gated embeddings in end-to-end speech recognition for conversational-context fusion",
          "conditional computation for continual learning",
          "learning in gated neural networks",
          "adapting neural networks for the estimation of treatment effects",
          "interpretable pid parameter tuning for control engineering using general dynamic neural networks: an extensive comparison",
          "robust sound event detection in bioacoustic sensor networks",
          "dynamic neural network channel execution for efficient training",
          "adaptive neural network based dynamic surface control for uncertain dual arm robots",
          "conditional channel gated networks for task-aware continual learning",
          "l\n0\n-arm: network sparsification via stochastic binary optimization",
          "on the functional equivalence of tsk fuzzy systems to neural networks, mixture of experts, cart, and stacking ensemble regression",
          "a mixture of experts model for predicting persistent weather patterns",
          "a new approach for topic detection using adaptive neural networks",
          "stefann: scene text editor using font adaptive neural network",
          "boundary-weighted domain adaptive neural network for prostate mr image segmentation",
          "uncertainty-aware driver trajectory prediction at urban intersections",
          "dropout regularization in hierarchical mixture of experts",
          "channel selection using gumbel softmax",
          "towards neural network patching: evaluating engagement-layers and patch-architectures",
          "spatiotemporal adaptive neural network for long-term forecasting of financial time series",
          "you look twice: gaternet for dynamic filter selection in cnns",
          "a mixture of expert approach for low-cost customization of deep neural networks",
          "shallow-deep networks: understanding and mitigating network overthinking",
          "mixture of expert/imitator networks: scalable semi-supervised learning framework",
          "multi-source cross-lingual model transfer: learning what to share",
          "human motion prediction using semi-adaptable neural networks",
          "deepdrum: an adaptive conditional neural network",
          "rate-adaptive neural networks for spatial multiplexers",
          "reservoir computing based neural image filters",
          "bounded rational decision-making with adaptive neural network priors",
          "fast adaptively weighted matrix factorization for recommendation with implicit feedback",
          "designing adaptive neural networks for energy-constrained image classification",
          "unsupervised domain adaptation by adversarial learning for robust speech recognition",
          "conditional information gain networks",
          "bayesian filtering unifies adaptive and non-adaptive neural network optimization methods",
          "adaptive neural trees",
          "context-adaptive neural network based prediction for image compression",
          "one-shot learning for question-answering in gaokao history challenge",
          "edge intelligence: on-demand deep learning model co-inference with device-edge synergy",
          "moe-spnet: a mixture-of-experts scene parsing network",
          "channel gating neural networks",
          "voice separation with an unknown number of multiple speakers",
          "adaptive neural network classifier for decoding meg signals",
          "dictionary learning by dynamical neural networks",
          "deep predictive coding network with local recurrent processing for object recognition",
          "a dynamic neural network approach to generating robot's novel actions: a simulation experiment",
          "view adaptive neural networks for high performance skeleton-based human action recognition",
          "espnet: end-to-end speech processing toolkit",
          "discontinuity-sensitive optimal control learning by mixture of experts",
          "breaking the gridlock in mixture-of-experts: consistent and efficient algorithms",
          "granger-causal attentive mixtures of experts: learning important features with neural networks",
          "overcoming the vanishing gradient problem in plain recurrent networks",
          "the tree ensemble layer: differentiability meets conditional computation",
          "topic compositional neural language model",
          "cavs: a vertex-centric programming interface for dynamic neural networks",
          "learning sparse neural networks through\nl\n0\nregularization",
          "deep gaussian covariance network",
          "fast yolo: a fast you only look once system for real-time embedded object detection in video",
          "branchynet: fast inference via early exiting from deep neural networks",
          "uts submission to google youtube-8m challenge 2017",
          "effective approaches to batch parallelization for dynamic neural network architectures",
          "large-scale youtube-8m video understanding with deep neural networks",
          "predictive coding-based deep dynamic neural network for visuomotor learning",
          "controlling computation versus quality for neural sequence models",
          "seamless integration and coordination of cognitive skills in humanoid robots: a deep learning approach",
          "a tale of two animats: what does it take to have goals?",
          "ampnet: asynchronous model-parallel training for dynamic neural networks",
          "on-the-fly operation batching in dynamic computation graphs",
          "multi-scale dense networks for resource efficient image classification",
          "speech enhancement using a deep mixture of experts",
          "quality resilient deep neural networks",
          "learning robust visual-semantic embeddings",
          "adaptive ensemble prediction for deep neural networks based on confidence level",
          "adaptive neural networks for efficient inference",
          "towards crowdsourced training of large neural networks using decentralized mixture-of-experts",
          "gated multimodal units for information fusion",
          "visual saliency prediction using a mixture of deep neural networks",
          "outrageously large neural networks: the sparsely-gated mixture-of-experts layer",
          "dynet: the dynamic neural network toolkit",
          "factory of realities: on the emergence of virtual spatiotemporal structures",
          "unifying multi-domain multi-task learning: tensor and neural network perspectives",
          "nonlinear systems identification using deep dynamic neural networks",
          "gated neural networks for option pricing: rationality by design",
          "deep online convex optimization with gated games",
          "decision forests, convolutional networks and the models in-between",
          "unsupervised adaptive neural network regularization for accelerated radial cine mri",
          "attractor metadynamics in adapting neural networks",
          "low-rank approximations for conditional feedforward computation in deep neural networks",
          "estimating or propagating gradients through stochastic neurons for conditional computation",
          "neurally implementable semantic networks",
          "extended mixture of mlp experts by hybrid of conjugate gradient method and modified cuckoo search",
          "electricity demand and energy consumption management system",
          "learning to bluff",
          "from neuron to neural networks dynamics",
          "from neuron to neural networks dynamics",
          "dynamical neural network: information and topology",
          "concept of e-machine: how does a \"dynamical\" brain learn to process \"symbolic\" information? part i",
          "conditional computation in neural networks for faster models",
          "machine learning methods to analyze arabidopsis thaliana plant root growth",
          "achieving synergy in cognitive behavior of humanoids via deep learning of dynamic visuo-motor-attentional coordination",
          "efficient large scale video classification",
          "distilling the knowledge in a neural network",
          "web spam classification using supervised artificial neural network algorithms",
          "pseudo dynamic transitional modeling of building heating energy demand using artificial neural network",
          "domain adaptive neural networks for object recognition",
          "exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning"
         ],
         "legendgroup": "",
         "marker": {
          "color": [
           8,
           15,
           5,
           20,
           21,
           6,
           26,
           2,
           1,
           1,
           14,
           1,
           7,
           9,
           8,
           4,
           15,
           16,
           20,
           1,
           1,
           6,
           19,
           13,
           14,
           5,
           20,
           5,
           7,
           5,
           5,
           17,
           3,
           16,
           4,
           28,
           9,
           7,
           24,
           2,
           21,
           28,
           9,
           15,
           14,
           26,
           16,
           17,
           18,
           7,
           21,
           4,
           12,
           12,
           3,
           6,
           1,
           16,
           0,
           1,
           10,
           13,
           3,
           7,
           9,
           5,
           24,
           24,
           22,
           28,
           21,
           8,
           21,
           19,
           14,
           8,
           20,
           4,
           19,
           17,
           5,
           14,
           22,
           22,
           26,
           1,
           15,
           7,
           5,
           8,
           15,
           21,
           28,
           16,
           2,
           5,
           12,
           20,
           7,
           11,
           8,
           17,
           14,
           28,
           20,
           2,
           6,
           28,
           8,
           15,
           3,
           8,
           10,
           10,
           9,
           10,
           5,
           21,
           26,
           7,
           21,
           10,
           12,
           16,
           3,
           7,
           12,
           21,
           12,
           9,
           0,
           16,
           2,
           5,
           24,
           23,
           8,
           2,
           17,
           26,
           16,
           4,
           15,
           10,
           21,
           21,
           13,
           16,
           20,
           7,
           27,
           17,
           8,
           20,
           0,
           15,
           0,
           5,
           12,
           0,
           24,
           15,
           8,
           20,
           11,
           25,
           13,
           21,
           3,
           11,
           26,
           8,
           4,
           2,
           2,
           17,
           24,
           0,
           12,
           11,
           20,
           26,
           19,
           26,
           0,
           28,
           1,
           15,
           15,
           25,
           1,
           2,
           8,
           17,
           3,
           26,
           7,
           4,
           15,
           1,
           6,
           18,
           1,
           10,
           13,
           7,
           15,
           26,
           3,
           1,
           18,
           20,
           22,
           24,
           27,
           17,
           20,
           24,
           18,
           27,
           17,
           0,
           17,
           9,
           24,
           0,
           10,
           1,
           1,
           0,
           5,
           28,
           13,
           13,
           20,
           26,
           16,
           15,
           10,
           0,
           13,
           20,
           6,
           3,
           23,
           28,
           10,
           13,
           9,
           9,
           20,
           12,
           7,
           12,
           14,
           27,
           20,
           24,
           17,
           0,
           12,
           27,
           3,
           2,
           2,
           14,
           9,
           10,
           7,
           9,
           3,
           8,
           7,
           1,
           14,
           1,
           20,
           17,
           0,
           2,
           7,
           14,
           15,
           10,
           28,
           12,
           9,
           9,
           4,
           5,
           10,
           17,
           25,
           23,
           12,
           3,
           9,
           13,
           17,
           24,
           11,
           17,
           9,
           25,
           2,
           11,
           6,
           18,
           18,
           20,
           2,
           17,
           18,
           0,
           1,
           24,
           19,
           11,
           20,
           17
          ],
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          -0.6197860836982727,
          -0.03816152364015579,
          0.7733451724052429,
          0.08502285182476044,
          -1.248543620109558,
          -1.6143920421600342,
          -0.7169650793075562,
          0.7004142999649048,
          -0.8256050944328308,
          -0.7245250940322876,
          -0.7305475473403931,
          -0.833903431892395,
          -0.7560853362083435,
          0.363887757062912,
          -1.0801308155059814,
          -1.0235768556594849,
          -0.22046193480491638,
          -1.3140548467636108,
          -0.03177378699183464,
          -0.5022649765014648,
          -0.4275204539299011,
          -1.349358081817627,
          0.7433791756629944,
          -0.1282670497894287,
          -0.6089944243431091,
          0.5196738839149475,
          -0.1283053606748581,
          0.8992250561714172,
          -0.9406269788742065,
          0.9787037372589111,
          0.9584624767303467,
          -0.03084034100174904,
          0.877066433429718,
          -1.8232027292251587,
          -0.9219289422035217,
          -0.24892163276672363,
          0.4914872646331787,
          -0.5290703177452087,
          0.31490465998649597,
          0.15814383327960968,
          -1.1768585443496704,
          -0.3720770478248596,
          0.16415424644947052,
          -0.23560526967048645,
          -0.5462294816970825,
          -0.786880373954773,
          -1.2674614191055298,
          -0.24285238981246948,
          1.0294654369354248,
          -0.7200528383255005,
          -1.241092562675476,
          -0.7957487106323242,
          -0.1522940695285797,
          0.11998626589775085,
          1.1130937337875366,
          -1.4942413568496704,
          -0.7846636176109314,
          -1.5238116979599,
          1.4137260913848877,
          -0.41650909185409546,
          0.041982341557741165,
          0.2490409016609192,
          0.6257588267326355,
          -0.6121861934661865,
          0.397468239068985,
          0.46873509883880615,
          0.10986142605543137,
          0.31008362770080566,
          0.709314227104187,
          -0.4357617497444153,
          -1.1050302982330322,
          -0.5770852565765381,
          -1.0576163530349731,
          0.758133590221405,
          -0.6051432490348816,
          -0.6137366890907288,
          0.04098177328705788,
          -0.90273517370224,
          0.5772020816802979,
          -0.10762977600097656,
          0.6626502871513367,
          -0.39259999990463257,
          0.29105812311172485,
          0.21660321950912476,
          -0.8596328496932983,
          -0.8997107148170471,
          -0.2637360394001007,
          -0.7115474343299866,
          0.5955159068107605,
          -0.9166803956031799,
          -0.4174037277698517,
          -1.1435320377349854,
          -0.17425090074539185,
          -1.326768159866333,
          0.47083592414855957,
          0.9721983075141907,
          0.22940781712532043,
          0.13142789900302887,
          -0.6784631013870239,
          0.964492678642273,
          -0.9276522397994995,
          -0.09724844992160797,
          -0.43287393450737,
          -0.196237713098526,
          -0.05851319804787636,
          0.6235240697860718,
          -1.3991963863372803,
          -0.4395178258419037,
          -0.6870669722557068,
          -0.3015154302120209,
          1.124043345451355,
          -0.6138020753860474,
          0.22833620011806488,
          0.30075016617774963,
          0.251644492149353,
          0.24366135895252228,
          0.8538983464241028,
          -1.059775471687317,
          -0.6709254384040833,
          -0.6522864103317261,
          -1.1490998268127441,
          0.49558448791503906,
          0.399685800075531,
          -1.3775733709335327,
          0.5405299067497253,
          -0.9034950137138367,
          0.3350110352039337,
          -1.393270492553711,
          0.04085731878876686,
          0.04678722470998764,
          1.5257004499435425,
          -1.3963698148727417,
          0.32685014605522156,
          0.5726383924484253,
          0.0779976174235344,
          1.1105350255966187,
          -0.5910825133323669,
          0.2218693345785141,
          -0.3012109100818634,
          -0.581546425819397,
          -1.4373373985290527,
          -1.005431056022644,
          -0.14691251516342163,
          0.08312549442052841,
          -1.184790849685669,
          -1.2586263418197632,
          -0.26543429493904114,
          -1.555140495300293,
          0.15367461740970612,
          -0.3926767408847809,
          -0.10786885023117065,
          -0.00698735099285841,
          -0.5640016198158264,
          0.23600588738918304,
          1.5712876319885254,
          -0.2941015958786011,
          1.1854732036590576,
          0.6076416373252869,
          -0.16761937737464905,
          1.5712640285491943,
          0.11162485927343369,
          -0.1096600666642189,
          -0.4364825487136841,
          0.18417377769947052,
          0.9706710577011108,
          -0.6922743320465088,
          0.0227893628180027,
          -1.1062779426574707,
          0.5236113667488098,
          0.8344452381134033,
          -0.7139759659767151,
          -0.5425199866294861,
          -0.9427476525306702,
          0.8250154256820679,
          0.7766558527946472,
          -0.14401645958423615,
          0.3126566708087921,
          1.1408590078353882,
          0.2620044946670532,
          1.1594903469085693,
          -0.009994620457291603,
          -0.6202163100242615,
          0.6821349263191223,
          -0.8837182521820068,
          1.5370969772338867,
          -0.39667221903800964,
          -0.5629525184631348,
          -0.3252412676811218,
          -0.05413760617375374,
          -0.7177926301956177,
          -1.047285556793213,
          0.38103052973747253,
          -0.888990044593811,
          -0.10524038225412369,
          0.6134436130523682,
          -0.7027021646499634,
          -0.5132327079772949,
          -1.0070563554763794,
          -0.1447085440158844,
          -0.5025407075881958,
          -1.6288546323776245,
          1.3212970495224,
          -0.6052327752113342,
          0.08473380655050278,
          -0.13064268231391907,
          -0.8615337014198303,
          -0.01959758996963501,
          -0.5900236964225769,
          1.0398918390274048,
          -0.618799090385437,
          1.613512396812439,
          -0.11789195239543915,
          0.45674505829811096,
          0.18141056597232819,
          -0.003648027079179883,
          -0.12347870320081711,
          0.1658027321100235,
          0.4128161370754242,
          1.3657158613204956,
          0.3725601136684418,
          -0.24449048936367035,
          1.4867902994155884,
          -0.23386181890964508,
          0.3812847137451172,
          0.4494961202144623,
          1.4866552352905273,
          0.24819596111774445,
          -0.548823893070221,
          -0.9039690494537354,
          1.4046599864959717,
          0.8589584827423096,
          -0.28843945264816284,
          -0.05783881992101669,
          0.11361581087112427,
          -0.11642392724752426,
          -0.7147074937820435,
          -1.2479742765426636,
          -0.26612967252731323,
          0.0773669108748436,
          1.2126353979110718,
          -0.057826440781354904,
          -0.047443050891160965,
          -1.2408770322799683,
          0.6855344176292419,
          1.4356296062469482,
          -0.18816758692264557,
          0.20203666388988495,
          -0.1276891976594925,
          0.6301538944244385,
          0.29243597388267517,
          -0.034813180565834045,
          0.10630137473344803,
          -0.871517539024353,
          0.04156392067670822,
          -0.32611411809921265,
          0.37837788462638855,
          0.019880738109350204,
          0.2730019986629486,
          -0.36142730712890625,
          1.5551589727401733,
          -0.13097907602787018,
          0.036142654716968536,
          0.864294707775116,
          0.5464174747467041,
          0.30522263050079346,
          -0.5558649897575378,
          0.010583675466477871,
          0.31788405776023865,
          -0.6308023929595947,
          0.4549402594566345,
          0.6637406945228577,
          -0.9068311452865601,
          -0.8739335536956787,
          -0.46848055720329285,
          -0.42470622062683105,
          -0.6976937651634216,
          0.323903352022171,
          -0.11297998577356339,
          1.2178490161895752,
          0.8316211104393005,
          -0.5476351976394653,
          -0.6649514436721802,
          -0.29835107922554016,
          0.1919945925474167,
          -0.2142731249332428,
          -0.284461110830307,
          0.24054473638534546,
          0.22147920727729797,
          -1.171143889427185,
          0.5049780607223511,
          -0.17215856909751892,
          -0.15558971464633942,
          -0.7375389933586121,
          1.230968713760376,
          -0.05186633765697479,
          0.6100722551345825,
          0.568355917930603,
          0.2411104291677475,
          0.03229706734418869,
          0.19742098450660706,
          0.9055111408233643,
          -0.04296383261680603,
          0.43278491497039795,
          -0.4859791696071625,
          0.3583293855190277,
          0.9357971549034119,
          -1.886488437652588,
          1.1009007692337036,
          1.100900650024414,
          0.08175820112228394,
          0.6622644662857056,
          -0.10188089311122894,
          1.1511282920837402,
          1.0106432437896729,
          -0.875952959060669,
          0.0450417585670948,
          0.849639356136322,
          1.1033498048782349,
          0.24827495217323303,
          -0.05808551609516144
         ],
         "xaxis": "x",
         "y": [
          -0.6073221564292908,
          -0.636702835559845,
          -0.7867878675460815,
          -0.001614921959117055,
          0.11894137412309647,
          -0.764140784740448,
          -0.20531967282295227,
          -0.2966480255126953,
          -1.3516203165054321,
          -1.3338887691497803,
          0.8274855017662048,
          -1.2466058731079102,
          0.06152218580245972,
          0.5152779221534729,
          -0.5802451372146606,
          0.7500014305114746,
          -0.4865289628505707,
          0.26290416717529297,
          -0.15057213604450226,
          -1.1043472290039062,
          -1.6028977632522583,
          -1.1737529039382935,
          -1.3834623098373413,
          1.0880569219589233,
          0.6571928262710571,
          -0.5750669836997986,
          -0.013633012771606445,
          -0.6888408660888672,
          0.14945325255393982,
          -0.7643778324127197,
          -0.7717828750610352,
          0.18094536662101746,
          -0.128350168466568,
          0.05776510760188103,
          0.9057153463363647,
          -0.039239220321178436,
          0.747678816318512,
          0.05898400396108627,
          0.20980378985404968,
          -0.392002135515213,
          0.17127777636051178,
          -0.2138434648513794,
          0.6540337204933167,
          -0.5735758543014526,
          0.7609289884567261,
          -0.16988612711429596,
          0.5182547569274902,
          0.08351065218448639,
          0.35505709052085876,
          0.3613465130329132,
          -0.08533533662557602,
          0.5435814261436462,
          -0.9068958759307861,
          -1.0003396272659302,
          0.05409040302038193,
          -0.5831633806228638,
          -1.3125360012054443,
          0.6061067581176758,
          -0.5540769100189209,
          -1.4786819219589233,
          -1.2354601621627808,
          0.8428934216499329,
          -0.07534366846084595,
          0.15784931182861328,
          0.42492392659187317,
          -0.6028583645820618,
          0.1529727578163147,
          0.18594031035900116,
          1.11365807056427,
          -0.07989199459552765,
          -0.033158548176288605,
          -0.7161427736282349,
          0.12120009958744049,
          -1.5828710794448853,
          0.6521750688552856,
          -0.96571284532547,
          -0.10053759068250656,
          0.9566534757614136,
          -1.6246845722198486,
          0.31743478775024414,
          -0.6540931463241577,
          0.5035144090652466,
          1.4937325716018677,
          1.3159005641937256,
          -0.2687869369983673,
          -1.4689490795135498,
          -0.6089377403259277,
          0.21665050089359283,
          -0.8681117296218872,
          -0.7400330901145935,
          -0.4434213936328888,
          0.05078014358878136,
          -0.05046457424759865,
          0.19655592739582062,
          -0.2761393189430237,
          -0.8481594324111938,
          -0.8341038227081299,
          -0.078072190284729,
          0.32993605732917786,
          1.3612711429595947,
          -0.8563083410263062,
          0.38622337579727173,
          0.5984380841255188,
          -0.021157031878829002,
          0.029728775843977928,
          -0.36999422311782837,
          -1.0324609279632568,
          -0.17011240124702454,
          -0.7402588129043579,
          -0.3414982557296753,
          0.16480910778045654,
          -0.4258393347263336,
          -1.0922763347625732,
          -1.2547886371612549,
          0.46051499247550964,
          -1.3351844549179077,
          -0.5928346514701843,
          0.13484568893909454,
          -0.20930953323841095,
          0.14699161052703857,
          -0.016148248687386513,
          -1.2029991149902344,
          -0.8354869484901428,
          0.333194762468338,
          -0.02069864049553871,
          0.06837236881256104,
          -0.7976816892623901,
          -0.006475093774497509,
          -0.9329577684402466,
          0.5352122783660889,
          -0.407378226518631,
          0.2775527536869049,
          -0.2932542860507965,
          -0.7600554823875427,
          0.2466185986995697,
          -0.9674428105354309,
          -0.46370574831962585,
          -0.5093448758125305,
          0.34208181500434875,
          -0.19475966691970825,
          0.18463601171970367,
          0.43472304940223694,
          -0.4407491087913513,
          -1.3385250568389893,
          -0.27856242656707764,
          -0.07892658561468124,
          0.7966160178184509,
          0.2652069330215454,
          -0.13461317121982574,
          0.08739858865737915,
          -1.7347745895385742,
          0.27743127942085266,
          -0.5953987240791321,
          0.030852969735860825,
          -0.06205252930521965,
          -0.3470782935619354,
          -0.24651196599006653,
          -0.5533450841903687,
          -0.8153331875801086,
          -0.5138449668884277,
          0.31932884454727173,
          -0.6632536053657532,
          -0.9385694861412048,
          -0.10219644010066986,
          0.8940935730934143,
          1.4286755323410034,
          0.8533962368965149,
          -0.2891692817211151,
          -0.07796774804592133,
          0.9613476395606995,
          -0.1142973005771637,
          -0.8576894998550415,
          0.5009835958480835,
          -0.22841913998126984,
          -0.39322853088378906,
          0.3131822645664215,
          0.21620972454547882,
          -0.48440247774124146,
          -0.930803120136261,
          0.6834116578102112,
          -0.1999455690383911,
          -0.17731644213199615,
          -1.770900011062622,
          -0.2244303822517395,
          -0.021646248176693916,
          -0.00583979906514287,
          -1.3027915954589844,
          -0.46710050106048584,
          -0.6574517488479614,
          1.161948323249817,
          -1.236818552017212,
          -0.28637272119522095,
          -0.796168327331543,
          0.20859681069850922,
          0.2256205528974533,
          -0.14085766673088074,
          0.16628289222717285,
          0.36513495445251465,
          -0.43927398324012756,
          -1.2898118495941162,
          -0.9248759150505066,
          0.48522433638572693,
          -1.8949182033538818,
          -1.5109221935272217,
          0.9066421389579773,
          -0.009175773710012436,
          -0.5538501143455505,
          -0.2857258915901184,
          0.10088517516851425,
          -1.14798104763031,
          0.600324273109436,
          -0.22379182279109955,
          1.2230517864227295,
          0.28667834401130676,
          -1.6380600929260254,
          0.31235697865486145,
          0.0184301920235157,
          0.24559445679187775,
          0.17495512962341309,
          -1.8315558433532715,
          0.31028249859809875,
          0.053708579391241074,
          0.19868558645248413,
          0.5549643039703369,
          0.19017603993415833,
          -0.5167517066001892,
          -1.2251944541931152,
          -1.5215574502944946,
          -1.7603193521499634,
          -0.525267481803894,
          -0.7074589133262634,
          -0.10406627506017685,
          0.9876760840415955,
          0.913562536239624,
          0.09856688976287842,
          -0.058822449296712875,
          0.4883778989315033,
          -0.23838023841381073,
          -1.2867541313171387,
          -0.3740536868572235,
          1.1099371910095215,
          -0.29989174008369446,
          -1.1190972328186035,
          0.18566010892391205,
          -1.047381043434143,
          -0.11088333278894424,
          -1.292230486869812,
          0.7674628496170044,
          0.5065626502037048,
          0.46571457386016846,
          -0.009311909787356853,
          -0.8178658485412598,
          -0.035714589059352875,
          -1.0257885456085205,
          0.5921978950500488,
          -1.7855526208877563,
          0.02644932270050049,
          0.14303001761436462,
          0.33861586451530457,
          -0.03906126320362091,
          -0.8178378343582153,
          -1.8220502138137817,
          -0.013437925837934017,
          -0.4193229675292969,
          -0.45968931913375854,
          0.8031534552574158,
          0.5329848527908325,
          -1.2215994596481323,
          0.4152142107486725,
          0.8483502864837646,
          0.04739224538207054,
          -0.6496373414993286,
          0.25541359186172485,
          -1.3357456922531128,
          0.4694390594959259,
          -1.2360327243804932,
          -0.12688285112380981,
          0.2653689682483673,
          -0.3394332230091095,
          -0.3148554563522339,
          0.3974684476852417,
          0.8278043866157532,
          -0.6246132254600525,
          -1.3980001211166382,
          -0.06869497895240784,
          -0.8395413160324097,
          0.6932770013809204,
          0.5126364231109619,
          0.8290368318557739,
          -0.5940712690353394,
          -1.3016026020050049,
          0.41455018520355225,
          1.1802875995635986,
          -1.0018579959869385,
          -0.8795727491378784,
          -0.0007450791890732944,
          0.6525505781173706,
          0.8910430073738098,
          0.2906191945075989,
          0.29813411831855774,
          0.7027525901794434,
          0.19777733087539673,
          0.5883522629737854,
          1.0451749563217163,
          -0.4846455454826355,
          0.9491031169891357,
          -0.935790479183197,
          0.4037349224090576,
          0.40373486280441284,
          0.06092330068349838,
          -0.36445292830467224,
          0.33479809761047363,
          0.3659173548221588,
          -0.41413360834121704,
          -1.3599649667739868,
          0.2304222285747528,
          -1.3253041505813599,
          1.1279903650283813,
          -0.17670388519763947,
          0.29214534163475037
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x_tsne"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y_tsne"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", 180)\n",
    "display(runs_df.sort_values(\"Silhouette\", ascending=False).iloc[:180])\n",
    "\n",
    "plt_data = pd.DataFrame(\n",
    "    {\n",
    "        \"x_tsne\": X_tsne[:, 0],\n",
    "        \"y_tsne\": X_tsne[:, 1],\n",
    "        \"content\": titles[:len(X_tsne)],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt_data[\"label\"] = labels\n",
    "fig = px.scatter(\n",
    "    plt_data,\n",
    "    x=\"x_tsne\",\n",
    "    y=\"y_tsne\",\n",
    "    hover_name=\"content\",\n",
    "    hover_data=[\"label\"],\n",
    "    color=labels,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict as dd\n",
    "\n",
    "cluster_counts = dd(int)\n",
    "cluster_titles = dd(list)\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    cluster_counts[label] += 1\n",
    "    cluster_titles[label].append(titles[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clusters = sorted(cluster_titles.items(), key=lambda x: -len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23,\n",
       "  ['mixture of elm based experts with trainable gating network',\n",
       "   'fast adaptively weighted matrix factorization for recommendation with implicit feedback',\n",
       "   'factory of realities: on the emergence of virtual spatiotemporal structures']),\n",
       " (25,\n",
       "  ['dynamically throttleable neural networks (tnn)',\n",
       "   'gshard: scaling giant models with conditional computation and automatic sharding',\n",
       "   'dynet: the dynamic neural network toolkit',\n",
       "   'neurally implementable semantic networks']),\n",
       " (22,\n",
       "  ['sparsity-constrained optimal transport',\n",
       "   'test-time adaptation via conjugate pseudo-labels',\n",
       "   'eve: environmental adaptive neural network models for low-power energy harvesting system',\n",
       "   'an empirical study of batch normalization and group normalization in conditional computation']),\n",
       " (27,\n",
       "  ['siamese labels auxiliary learning',\n",
       "   'gated embeddings in end-to-end speech recognition for conversational-context fusion',\n",
       "   'robust sound event detection in bioacoustic sensor networks',\n",
       "   'voice separation with an unknown number of multiple speakers',\n",
       "   'espnet: end-to-end speech processing toolkit']),\n",
       " (19,\n",
       "  ['improving deep attractor network by bgru and gmm for speech separation',\n",
       "   'a new hazard event classification model via deep learning and multifractal',\n",
       "   'admoe: anomaly detection with mixture-of-experts from noisy labels',\n",
       "   'mixcaps: a capsule network-based mixture of experts for lung nodule malignancy prediction',\n",
       "   'web spam classification using supervised artificial neural network algorithms']),\n",
       " (11,\n",
       "  ['stock price prediction using dynamic neural networks',\n",
       "   'discovery of the hidden state in ionic models using a domain-specific recurrent neural network',\n",
       "   'adapting neural networks for uplift models',\n",
       "   'adaptive neural network-based approximation to accelerate eulerian fluid simulation',\n",
       "   'attractor metadynamics in adapting neural networks',\n",
       "   'electricity demand and energy consumption management system',\n",
       "   'pseudo dynamic transitional modeling of building heating energy demand using artificial neural network']),\n",
       " (18,\n",
       "  ['adaptive neural network backstepping control method for aerial manipulator based on variable inertia parameter modeling',\n",
       "   'learning charme models with neural networks',\n",
       "   'an adaptive architecture for portability of greenhouse models',\n",
       "   'interpretable pid parameter tuning for control engineering using general dynamic neural networks: an extensive comparison',\n",
       "   'from neuron to neural networks dynamics',\n",
       "   'from neuron to neural networks dynamics',\n",
       "   'machine learning methods to analyze arabidopsis thaliana plant root growth']),\n",
       " (6,\n",
       "  ['csi-based efficient self-quarantine monitoring system using branchy convolution neural network',\n",
       "   'dynamic healthcare embeddings for improving patient care',\n",
       "   'alioth: a machine learning based interference-aware performance monitor for multi-tenancy applications in public cloud',\n",
       "   'mobilefaceswap: a lightweight framework for video face swapping',\n",
       "   'deepcare: a deep dynamic memory model for predictive medicine',\n",
       "   'reservoir computing based neural image filters',\n",
       "   'learning to bluff']),\n",
       " (4,\n",
       "  ['graph mixture of experts: learning on large-scale graphs with explicit diversity modeling',\n",
       "   'ed-batch: efficient automatic batching of dynamic neural networks via learned finite state machines',\n",
       "   'boosted dynamic neural networks',\n",
       "   'dycl: dynamic neural network compilation via program rewriting and graph optimization',\n",
       "   'dyno: dynamic onloading of deep neural networks from cloud to device',\n",
       "   'revisiting batch normalization for training low-latency deep spiking neural networks from scratch',\n",
       "   'why should we add early exits to neural networks?',\n",
       "   'towards crowdsourced training of large neural networks using decentralized mixture-of-experts']),\n",
       " (28,\n",
       "  ['enhancing once-for-all: a study on parallel blocks, skip connections and early exits',\n",
       "   'post-train adaptive u-net for image segmentation',\n",
       "   'tuning of mixture-of-experts mixed-precision neural networks',\n",
       "   'exploring the intersection between neural architecture search and continual learning',\n",
       "   \"physics-guided problem decomposition for scaling deep learning of high-dimensional eigen-solvers: the case of schrödinger's equation\",\n",
       "   'an adaptive device-edge co-inference framework based on soft actor-critic',\n",
       "   'wrapnet: neural net inference with ultra-low-resolution arithmetic',\n",
       "   'channel selection using gumbel softmax',\n",
       "   'designing adaptive neural networks for energy-constrained image classification',\n",
       "   'quality resilient deep neural networks']),\n",
       " (13,\n",
       "  ['memorization capacity of neural networks with conditional computation',\n",
       "   'adaptive neural network ensemble using frequency distribution',\n",
       "   'task-adaptive neural network search with meta-contrastive learning',\n",
       "   'adaptive neural network-based ofdm receivers',\n",
       "   'hierarchical mixtures of generators for adversarial learning',\n",
       "   'towards neural network patching: evaluating engagement-layers and patch-architectures',\n",
       "   'spatiotemporal adaptive neural network for long-term forecasting of financial time series',\n",
       "   'deepdrum: an adaptive conditional neural network',\n",
       "   'conditional information gain networks',\n",
       "   'deep online convex optimization with gated games']),\n",
       " (16,\n",
       "  ['seenn: towards temporal spiking early-exit neural networks',\n",
       "   'lgvit: dynamic early exiting for accelerating vision transformer',\n",
       "   'quicknets: saving training and preventing overconfidence in early-exit neural architectures',\n",
       "   'cold start streaming learning for deep networks',\n",
       "   'continuer: maintaining distributed dnn services during edge failures',\n",
       "   'eeea-net: an early exit evolutionary neural architecture search',\n",
       "   'zero time waste: recycling predictions in early exit neural networks',\n",
       "   'improving the accuracy of early exits in multi-exit architectures via curriculum learning',\n",
       "   'e\\n2\\ncm: early exit via class means for efficient supervised and unsupervised learning',\n",
       "   'shallow-deep networks: understanding and mitigating network overthinking']),\n",
       " (14,\n",
       "  ['big-little adaptive neural networks on low-power near-subthreshold processors',\n",
       "   'gated compression layers for efficient always-on models',\n",
       "   'monadic deep learning',\n",
       "   'a survey of neural trees',\n",
       "   'on-demand resource management for 6g wireless networks using knowledge-assisted dynamic neural networks',\n",
       "   'ereba: black-box energy testing of adaptive neural networks',\n",
       "   'channel gating neural networks',\n",
       "   'overcoming the vanishing gradient problem in plain recurrent networks',\n",
       "   'effective approaches to batch parallelization for dynamic neural network architectures',\n",
       "   'on-the-fly operation batching in dynamic computation graphs']),\n",
       " (26,\n",
       "  ['marsellus: a heterogeneous risc-v ai-iot end-node soc with 2-to-8b dnn acceleration and 30%-boost adaptive body biasing',\n",
       "   'accuracy-guaranteed collaborative dnn inference in industrial iot via deep reinforcement learning',\n",
       "   't-recx: tiny-resource efficient convolutional neural networks with early-exit',\n",
       "   'dystyle: dynamic neural network for multi-attribute-conditioned style editing',\n",
       "   'ecnns: ensemble learning methods for improving planar grasp quality estimation',\n",
       "   'calibration-aided edge inference offloading via adaptive model partitioning of deep neural networks',\n",
       "   'spinn: synergistic progressive inference of neural networks over device and cloud',\n",
       "   'hapi: hardware-aware progressive inference',\n",
       "   'conditionally deep hybrid neural networks across edge and cloud',\n",
       "   'learning sparse mixture of experts for visual question answering',\n",
       "   'a mixture of expert approach for low-cost customization of deep neural networks']),\n",
       " (24,\n",
       "  ['neural abstractions',\n",
       "   'trainability, expressivity and interpretability in gated neural odes',\n",
       "   'probabilistic partition of unity networks for high-dimensional regression problems',\n",
       "   'student performance prediction using dynamic neural models',\n",
       "   'diffprune: neural network pruning with deterministic approximate binary gates and\\nl\\n0\\nregularization',\n",
       "   'aidx: adaptive inference scheme to mitigate state-drift in memristive vmm accelerators',\n",
       "   'controlling model complexity in probabilistic model-based dynamic optimization of neural network structures',\n",
       "   'adapting neural networks for the estimation of treatment effects',\n",
       "   'on the functional equivalence of tsk fuzzy systems to neural networks, mixture of experts, cart, and stacking ensemble regression',\n",
       "   'dictionary learning by dynamical neural networks',\n",
       "   'unsupervised adaptive neural network regularization for accelerated radial cine mri',\n",
       "   'distilling the knowledge in a neural network']),\n",
       " (3,\n",
       "  ['gaussian process-gated hierarchical mixtures of experts',\n",
       "   'physics-informed koopman network',\n",
       "   'on the adversarial robustness of mixture of experts',\n",
       "   'learning over all stabilizing nonlinear controllers for a partially-observed linear system',\n",
       "   'forecasting the outcome of spintronic experiments with neural ordinary differential equations',\n",
       "   'towards a universal gating network for mixtures of experts',\n",
       "   'surprisal-triggered conditional computation with neural networks',\n",
       "   'mpc-net: a first principles guided policy search',\n",
       "   'bounded rational decision-making with adaptive neural network priors',\n",
       "   'discontinuity-sensitive optimal control learning by mixture of experts',\n",
       "   'deep gaussian covariance network',\n",
       "   'nonlinear systems identification using deep dynamic neural networks']),\n",
       " (21,\n",
       "  ['towards anytime classification in early-exit architectures by enforcing conditional monotonicity',\n",
       "   'eenet: learning to early exit for adaptive inference',\n",
       "   'understanding the robustness of multi-exit models under common corruptions',\n",
       "   'fluid batching: exit-aware preemptive serving of early-exit neural networks on edge npus',\n",
       "   'improving the performance of dnn-based software services using automated layer caching',\n",
       "   'resource-constrained edge ai with early exit prediction',\n",
       "   'consistency training of multi-exit architectures for sensor data',\n",
       "   'early-exit deep neural networks for distorted images: providing an efficient edge offloading',\n",
       "   'multi-exit vision transformer for dynamic inference',\n",
       "   'resource allocation for multiuser edge inference with batching and early exiting (extended version)',\n",
       "   'split computing and early exiting for deep learning applications: survey and research challenges',\n",
       "   'branchy-gnn: a device-edge co-inference framework for efficient point cloud processing']),\n",
       " (10,\n",
       "  ['parallel gated neural network with attention mechanism for speech enhancement',\n",
       "   'a mixture of expert based deep neural network for improved asr',\n",
       "   'privacy attacks for automatic speech recognition acoustic models in a federated learning framework',\n",
       "   'private language model adaptation for speech recognition',\n",
       "   'airex: neural network-based approach for air quality inference in unmonitored cities',\n",
       "   'deep dynamic neural network to trade-off between accuracy and diversity in a news recommender system',\n",
       "   'improving distant supervised relation extraction by dynamic neural network',\n",
       "   'a new approach for topic detection using adaptive neural networks',\n",
       "   'multi-source cross-lingual model transfer: learning what to share',\n",
       "   'unsupervised domain adaptation by adversarial learning for robust speech recognition',\n",
       "   'topic compositional neural language model',\n",
       "   'speech enhancement using a deep mixture of experts',\n",
       "   'visual saliency prediction using a mixture of deep neural networks']),\n",
       " (12,\n",
       "  ['cfnet: conditional filter learning with dynamic noise estimation for real image denoising',\n",
       "   'spatial mixture-of-experts',\n",
       "   'interpretable mixture of experts',\n",
       "   'table-based fact verification with self-adaptive mixture of experts',\n",
       "   'federated mixture of experts',\n",
       "   'mixtures of deep neural experts for automated speech scoring',\n",
       "   \"self-supervised multimodal domino: in search of biomarkers for alzheimer's disease\",\n",
       "   'anomaly detection by recombining gated unsupervised experts',\n",
       "   'one-shot learning for question-answering in gaokao history challenge',\n",
       "   'moe-spnet: a mixture-of-experts scene parsing network',\n",
       "   'view adaptive neural networks for high performance skeleton-based human action recognition',\n",
       "   'learning robust visual-semantic embeddings',\n",
       "   'unifying multi-domain multi-task learning: tensor and neural network perspectives']),\n",
       " (0,\n",
       "  ['safe real-world autonomous driving by learning to predict and plan with a mixture of experts',\n",
       "   'machine learning methods for postprocessing ensemble forecasts of wind gusts: a systematic comparison',\n",
       "   'precise motion control of wafer stages via adaptive neural network and fractional-order super-twisting algorithm',\n",
       "   'a novel cluster classify regress model predictive controller formulation; ccr-mpc',\n",
       "   'multi-expert learning of adaptive legged locomotion',\n",
       "   'flash: fast and light motion prediction for autonomous driving with bayesian inverse planning and learned motion profiles',\n",
       "   'velocity regulation of 3d bipedal walking robots with uncertain dynamics through adaptive neural network controller',\n",
       "   'adaptive neural network based dynamic surface control for uncertain dual arm robots',\n",
       "   'a mixture of experts model for predicting persistent weather patterns',\n",
       "   'uncertainty-aware driver trajectory prediction at urban intersections',\n",
       "   'human motion prediction using semi-adaptable neural networks',\n",
       "   \"a dynamic neural network approach to generating robot's novel actions: a simulation experiment\",\n",
       "   'seamless integration and coordination of cognitive skills in humanoid robots: a deep learning approach',\n",
       "   'achieving synergy in cognitive behavior of humanoids via deep learning of dynamic visuo-motor-attentional coordination']),\n",
       " (2,\n",
       "  ['towards convergence rates for parameter estimation in gaussian-gated mixture of experts',\n",
       "   'adaensemble: learning adaptively sparse structured ensemble network for click-through rate prediction',\n",
       "   'uni-perceiver-moe: learning sparse generalist models with conditional moes',\n",
       "   'towards lightweight neural animation : exploration of neural network pruning in mixture of experts-based animation models',\n",
       "   'dselect-k: differentiable selection in the mixture of experts with applications to multi-task learning',\n",
       "   'kdexplainer: a task-oriented attention model for explaining knowledge distillation',\n",
       "   'identification of probability weighted arx models with arbitrary domains',\n",
       "   'flight-connection prediction for airline crew scheduling to construct initial clusters for or optimizer',\n",
       "   'fast deep mixtures of gaussian process experts',\n",
       "   'breaking the gridlock in mixture-of-experts: consistent and efficient algorithms',\n",
       "   'granger-causal attentive mixtures of experts: learning important features with neural networks',\n",
       "   'a tale of two animats: what does it take to have goals?',\n",
       "   'extended mixture of mlp experts by hybrid of conjugate gradient method and modified cuckoo search',\n",
       "   'concept of e-machine: how does a \"dynamical\" brain learn to process \"symbolic\" information? part i']),\n",
       " (5,\n",
       "  ['patch-level routing in mixture-of-experts is provably sample-efficient for convolutional neural networks',\n",
       "   'simultaneous action recognition and human whole-body motion and dynamics prediction from wearable sensors',\n",
       "   'a hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training',\n",
       "   'improving expert specialization in mixture of experts',\n",
       "   'ta-moe: topology-aware large scale mixture-of-expert training',\n",
       "   'feamoe: fair, explainable and adaptive mixture of experts',\n",
       "   'towards understanding mixture of experts in deep learning',\n",
       "   'the future of human-centric explainable artificial intelligence (xai) is not post-hoc explanations',\n",
       "   'adaptive neural network-based unscented kalman filter for robust pose tracking of noncooperative spacecraft',\n",
       "   'sparse moes meet efficient ensembles',\n",
       "   'moebert: from bert to mixture-of-experts via importance-guided adaptation',\n",
       "   'gated ensemble of spatio-temporal mixture of experts for multi-task learning in ride-hailing system',\n",
       "   'dropout regularization in hierarchical mixture of experts',\n",
       "   'gated multimodal units for information fusion']),\n",
       " (8,\n",
       "  ['energy-efficient and privacy-aware social distance monitoring with low-resolution infrared sensors and adaptive inference',\n",
       "   'dynamicdet: a unified dynamic architecture for object detection',\n",
       "   'sda-\\nx\\nnet: selective depth attention networks for adaptive multi-scale feature representation',\n",
       "   'human activity recognition on microcontrollers with quantized and adaptive deep neural networks',\n",
       "   'satbench: benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks',\n",
       "   'test-time adaptable neural networks for robust medical image segmentation',\n",
       "   'feature matching as improved transfer learning technique for wearable eeg',\n",
       "   'towards efficient single image dehazing and desnowing',\n",
       "   'show why the answer is correct! towards explainable ai using compositional temporal attention',\n",
       "   'learning task-oriented communication for edge inference: an information bottleneck approach',\n",
       "   'dadnn: multi-scene ctr prediction via domain-aware deep neural network',\n",
       "   'glance and focus: a dynamic approach to reducing spatial redundancy in image classification',\n",
       "   'elf: an early-exiting framework for long-tailed classification',\n",
       "   'fast yolo: a fast you only look once system for real-time embedded object detection in video']),\n",
       " (9,\n",
       "  ['gradmdm: adversarial attack on dynamic networks',\n",
       "   'fast, differentiable and sparse top-k: a convex analysis perspective',\n",
       "   'adaptive neural networks using residual fitting',\n",
       "   'dynamics-aware adversarial attack of adaptive neural networks',\n",
       "   'polynomial-spline neural networks with exact integrals',\n",
       "   'better training using weight-constrained stochastic dynamics',\n",
       "   'l\\n0\\n-arm: network sparsification via stochastic binary optimization',\n",
       "   'bayesian filtering unifies adaptive and non-adaptive neural network optimization methods',\n",
       "   'adaptive neural trees',\n",
       "   'the tree ensemble layer: differentiability meets conditional computation',\n",
       "   'learning sparse neural networks through\\nl\\n0\\nregularization',\n",
       "   'adaptive ensemble prediction for deep neural networks based on confidence level',\n",
       "   'adaptive neural networks for efficient inference',\n",
       "   'gated neural networks for option pricing: rationality by design',\n",
       "   'estimating or propagating gradients through stochastic neurons for conditional computation']),\n",
       " (15,\n",
       "  ['moduleformer: learning modular large language models from uncurated data',\n",
       "   'long-tailed visual recognition via self-heterogeneous integration with knowledge excavation',\n",
       "   'heterogeneous domain adaptation and equipment matching: dann-based alignment with cyclic supervision (dbacs)',\n",
       "   'scai: a spectral data classification framework with adaptive inference for the iot platform',\n",
       "   'fault-tolerant collaborative inference through the edge-prune framework',\n",
       "   'heterogeneous transformer: a scale adaptable neural network architecture for device activity detection',\n",
       "   'contextual hypernetworks for novel feature adaptation',\n",
       "   'understanding the influence of receptive field and network complexity in neural-network-guided tem image analysis',\n",
       "   'anytime prediction as a model of human reaction time',\n",
       "   'learning to learn parameterized classification networks for scalable input images',\n",
       "   'summareranker: a multi-task mixture-of-experts re-ranking framework for abstractive summarization',\n",
       "   'the right tool for the job: matching model and instance complexities',\n",
       "   'cross-modal subspace learning with scheduled adaptive margin constraints',\n",
       "   'mixture of expert/imitator networks: scalable semi-supervised learning framework',\n",
       "   'multi-scale dense networks for resource efficient image classification']),\n",
       " (7,\n",
       "  ['atheena: a toolflow for hardware early-exit network automation',\n",
       "   'hierarchical training of deep neural networks using early exiting',\n",
       "   'anticipate, ensemble and prune: improving convolutional neural networks via aggregated early exits',\n",
       "   'hadas: hardware-aware dynamic neural architecture search for edge performance scaling',\n",
       "   'a novel membership inference attack against dynamic neural networks by utilizing policy networks information',\n",
       "   'binary early-exit network for adaptive inference on low-resource devices',\n",
       "   'optimizing mixture of experts using dynamic recompilations',\n",
       "   'dynamic neural network architectural and topological adaptation and related methods -- a survey',\n",
       "   'few-shot and continual learning with attentive independent mechanisms',\n",
       "   'embedded knowledge distillation in depth-level dynamic neural network',\n",
       "   'faster depth-adaptive transformers',\n",
       "   'edge ai: on-demand accelerating deep neural network inference via edge computing',\n",
       "   'edge intelligence: on-demand deep learning model co-inference with device-edge synergy',\n",
       "   'cavs: a vertex-centric programming interface for dynamic neural networks',\n",
       "   'branchynet: fast inference via early exiting from deep neural networks',\n",
       "   'ampnet: asynchronous model-parallel training for dynamic neural networks']),\n",
       " (17,\n",
       "  ['fixing overconfidence in dynamic neural networks',\n",
       "   'sparse upcycling: training mixture-of-experts from dense checkpoints',\n",
       "   'a theoretical view on sparsely activated networks',\n",
       "   'a survey on dynamic neural networks for natural language processing',\n",
       "   'basisnet: two-stage model synthesis for efficient inference',\n",
       "   'dynamic neural networks: a survey',\n",
       "   'neurocoder: learning general-purpose computation using stored neural programs',\n",
       "   'nimble: efficiently compiling dynamic neural networks for model inference',\n",
       "   'conditional computation for continual learning',\n",
       "   'dynamic neural network channel execution for efficient training',\n",
       "   'conditional channel gated networks for task-aware continual learning',\n",
       "   'deep predictive coding network with local recurrent processing for object recognition',\n",
       "   'controlling computation versus quality for neural sequence models',\n",
       "   'outrageously large neural networks: the sparsely-gated mixture-of-experts layer',\n",
       "   'decision forests, convolutional networks and the models in-between',\n",
       "   'low-rank approximations for conditional feedforward computation in deep neural networks',\n",
       "   'conditional computation in neural networks for faster models',\n",
       "   'exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning']),\n",
       " (1,\n",
       "  ['hypere2vid: improving event-based video reconstruction via hypernetworks',\n",
       "   'transforming visual scene graphs to image captions',\n",
       "   'long-distance gesture recognition using dynamic neural networks',\n",
       "   're-iqa: unsupervised learning for image quality assessment in the wild',\n",
       "   'a small-scale switch transformer and nlp-based model for clinical narratives classification',\n",
       "   'dynamic-pix2pix: noise injected cgan for modeling input and target domain joint distributions with limited training data',\n",
       "   'tiny-attention adapter: contexts are more important than the number of parameters',\n",
       "   'omni-seg: a scale-aware dynamic network for renal pathological image segmentation',\n",
       "   'learning to compose hypercolumns for visual correspondence',\n",
       "   'early exit or not: resource-efficient blind quality enhancement for compressed images',\n",
       "   'dynamic instance domain adaptation',\n",
       "   'gla in mediaeval 2018 emotional impact of movies task',\n",
       "   'expert sample consensus applied to camera re-localization',\n",
       "   'stefann: scene text editor using font adaptive neural network',\n",
       "   'boundary-weighted domain adaptive neural network for prostate mr image segmentation',\n",
       "   'uts submission to google youtube-8m challenge 2017',\n",
       "   'large-scale youtube-8m video understanding with deep neural networks',\n",
       "   'efficient large scale video classification']),\n",
       " (20,\n",
       "  ['soft merging of experts with adaptive routing',\n",
       "   'evolving artificial neural networks to imitate human behaviour in shinobi iii : return of the ninja master',\n",
       "   'dynamic neural network for multi-task learning searching across diverse network topologies',\n",
       "   'efficient sparsely activated transformers',\n",
       "   'temporal domain generalization with drift-aware dynamic neural networks',\n",
       "   'a field of experts prior for adapting neural networks at test time',\n",
       "   'deep learning with a classifier system: initial results',\n",
       "   'a reproducibility study of \"augmenting genetic algorithms with deep neural networks for exploring the chemical space\"',\n",
       "   'nested mixture of experts: cooperative and competitive learning of hybrid dynamical system',\n",
       "   'making neural networks interpretable with attribution: application to implicit signals prediction',\n",
       "   'deep adaptive inference networks for single image super-resolution',\n",
       "   'learning in gated neural networks',\n",
       "   'you look twice: gaternet for dynamic filter selection in cnns',\n",
       "   'rate-adaptive neural networks for spatial multiplexers',\n",
       "   'context-adaptive neural network based prediction for image compression',\n",
       "   'adaptive neural network classifier for decoding meg signals',\n",
       "   'predictive coding-based deep dynamic neural network for visuomotor learning',\n",
       "   'dynamical neural network: information and topology',\n",
       "   'domain adaptive neural networks for object recognition'])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_clusters[::-1] #[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_lenghts = [len(x[1]) for x in sorted_clusters[1:]]\n",
    "print(sum(cluster_lenghts)) # [1:125]\n",
    "plt.hist(cluster_lenghts, bins=220)\n",
    "plt.grid()\n",
    "plt.title(\"Distribuição do No de artigos.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
