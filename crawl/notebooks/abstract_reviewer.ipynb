{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "source_dir = os.path.join(\"..\", \"arxiv_papers_infos\")\n",
    "papers_paths = glob(os.path.join(source_dir, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "paper_data = []\n",
    "for path in papers_paths:\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    paper_data.append(data)\n",
    "abstracts = [data[\"result\"][\"abstract\"] for data in paper_data]\n",
    "print(len(paper_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "api_key_path = os.path.join(\n",
    "    \"C:\\\\Users\", \"DXT6\", \"OneDrive - PETROBRAS\",\n",
    "    \"CENPES\", \"TEO\", \"keys\", \"OpenAI\", \"apikey.txt\"\n",
    ")\n",
    "\n",
    "with open(api_key_path, \"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Consider the following text:\n",
    "\"{abstract}\"\n",
    "\n",
    "Now, consider the following definition:\n",
    "\"A Dynamic Neural Network is a type of Artificial Neural Network able to change itâ€™s structure\n",
    "and/or weights outside training, potentially being more flexible, saving resources, and being more accurate.\"\n",
    "\n",
    "Now answer the following questions. For each question, give the number of the question and the\n",
    "letter of the answer followed by your arguments.\n",
    "\n",
    "1. What can change outside training of the presented neural network?\n",
    "(a) The network function composition (related to the computational graph) loaded into working memory.\n",
    "This includes the cases where a RNN runs a variable number of steps.\n",
    "(b) The values of network weights.\n",
    "(c) Quantization of weights and feature maps.\n",
    "(d) None of the above or not mentioned.\n",
    "\n",
    "2. How to change the network outside training?\n",
    "(a) The network is known based on the input instance prior to the target inference. For instance,\n",
    "the inference can exit early and also skip layers.\n",
    "(b) Sequentially as the network unrolls, as a function of the input and produced feature maps so far.\n",
    "In this case, it is possible to ignore (or assign attention scores to) portions of the feature maps\n",
    "(pixels, channels, regions, time slices) and to work with different spatial scales.\n",
    "(c) By changing parameters with an unsupervised loss over a batch of data examples after training.\n",
    "(d) None of the above or not mentioned.\n",
    "\n",
    "3. What are the mechanisms for training the given network?\n",
    "(a) End-to-end backpropagation.\n",
    "(b) Reinforcement Learning (non-differentiable objective).\n",
    "(c) Gradient estimates.\n",
    "(d) None of the above or not mentioned.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.error import APIConnectionError\n",
    "from time import sleep\n",
    "\n",
    "absidx = 4\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": prompt.format(abstract=abstracts[absidx])}\n",
    "]\n",
    "retries = 4\n",
    "for _ in range(retries):\n",
    "  try:\n",
    "    ans = openai.ChatCompletion.create(\n",
    "      # model=\"gpt-3.5-turbo\",\n",
    "      model=\"gpt-3.5-turbo-0613\",\n",
    "      # model=\"gpt-4-0613\",\n",
    "      # prompt=prompt.format(abstract=abstracts[0]),\n",
    "      messages=messages,\n",
    "      # max_tokens=2048,\n",
    "      # temperature=0\n",
    "    )\n",
    "  except APIConnectionError:\n",
    "    print(\"Re-trying\")\n",
    "    sleep(3)\n",
    "    continue\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modern predictive models are often deployed to environments in which computational budgets are dynamic. anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. however, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. to address this shortcoming, we propose an elegant post-hoc modification, based on the product-of-experts, that encourages an early-exit network to become gradually confident. this gives our deep models the property of conditional monotonicity in the prediction quality -- an essential stepping stone towards truly anytime predictive modeling using early-exit architectures. our empirical results on standard image-classification tasks demonstrate that such behaviors can be achieved while preserving competitive accuracy on average.\n",
      "\n",
      "1. The answer is (b) The values of network weights. In the given text, it is mentioned that the quality of predictions for individual data points is not guaranteed to improve with longer computation. This indicates that the weights of the network can change outside of training in order to improve prediction quality.\n",
      "\n",
      "2. The answer is (a) The network is known based on the input instance prior to the target inference. In the text, it is mentioned that early-exit neural networks can provide intermediate predictions at various stages throughout the network. This implies that the network can change based on the input instance to exit early or skip layers.\n",
      "\n",
      "3. The answer is (a) End-to-end backpropagation. Although not explicitly mentioned in the text, the mention of \"competitive accuracy\" implies that the network has been trained using an optimization method like end-to-end backpropagation to achieve good performance on image-classification tasks.\n"
     ]
    }
   ],
   "source": [
    "print(abstracts[absidx])\n",
    "print()\n",
    "print(ans[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
